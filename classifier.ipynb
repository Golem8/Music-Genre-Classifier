{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifier.ipynb",
      "provenance": [],
      "mount_file_id": "1GhqrkZTpxJinyOcB-NCrAnaggvFLHt3A",
      "authorship_tag": "ABX9TyMKDBIsA3Stxkg1KfJX6GG8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Golem8/Music-Genre-Classifier/blob/main/classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTT99TXKvanV"
      },
      "source": [
        "This notebook takes in a mp3 upload and converts it to a spectrogram and displays the model's resultant classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk2LtVbkvtwr",
        "outputId": "c947ef2e-7108-40cc-dc14-9b5f9aee5d55"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ_fIV09xOHW"
      },
      "source": [
        "pathToEvaluate = r'/content/000140.mp3'\n",
        "modelPath = r'/content/78.7valacc'"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeS2MTN7v86y",
        "outputId": "5a885840-6d84-44e6-fbe2-121f7d8aa266"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch.optim as optim \n",
        "\n",
        "!pip install pydub\n",
        "import pydub\n",
        "import librosa, librosa.display\n",
        "from pydub import AudioSegment \n",
        "from pydub.utils import make_chunks\n",
        "\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUY-WzB-wFSc"
      },
      "source": [
        "# get a pretrained network for transfer learning\n",
        "import torchvision.models as models\n",
        "densenet = models.densenet161(pretrained=True)\n",
        "\n",
        "class transfer_music_classifer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(transfer_music_classifer, self).__init__()\n",
        "    self.featureExtract = densenet.features\n",
        "    \n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(2208, 780),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(780, 240),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(240, 5),\n",
        "      nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, spectrogram):\n",
        "    features = self.featureExtract(spectrogram)\n",
        "\n",
        "    ######\n",
        "    # This code is normally built into densenet, but since we are splitting\n",
        "    # the features and classifier it must be put here\n",
        "    # Taken from here: \n",
        "    # https://pytorch.org/vision/stable/_modules/torchvision/models/densenet.html\n",
        "\n",
        "    # inplace = True means it modifies the input instead of allocating memory\n",
        "    # for an ouput. Saves on memory\n",
        "    out = nnf.relu(features, inplace=True)\n",
        "\n",
        "    # adaptive pool decides the stride and kernel size automatically to ensure\n",
        "    # the output has shape (x,x,1,1) regardless of input\n",
        "    out = nnf.adaptive_avg_pool2d(out, (1, 1))\n",
        "\n",
        "    # reshape output to be (1, x)\n",
        "    out = torch.flatten(out, 1)\n",
        "    ######    \n",
        "\n",
        "    out = self.classifier(out)\n",
        "    return out"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtgL8vGWvSSB",
        "outputId": "8ea04efb-5a06-4eb0-bfaa-fa51b677ab3d"
      },
      "source": [
        "model = transfer_music_classifer()\n",
        "state = torch.load(modelPath, map_location='cpu')\n",
        "model.load_state_dict(state)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXE7oZKBwQdG"
      },
      "source": [
        "# Create 2 second .wav files for the selected audio\n",
        "try:\n",
        "  shutil.rmtree('/content/chunks')\n",
        "except:\n",
        "  pass\n",
        "os.mkdir('/content/chunks')\n",
        "audio_object = AudioSegment.from_mp3(pathToEvaluate) \n",
        "chunk_length_ms = 2000   #2000 ms clips each \n",
        "chunks = make_chunks(audio_object,chunk_length_ms)\n",
        "for iteration, chunk in enumerate(chunks): \n",
        "  # if the chunks are not 2000 ms long (the last one may be shorter)\n",
        "  # discard them as they would\n",
        "  # mess up the creation of the spectrograms\n",
        "  if len(chunk) == 2000:\n",
        "    chunk.export(os.path.join('/content', 'chunks', str(iteration) + '.wav'), format=\"wav\")"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sN-io1W0mu0",
        "outputId": "6c23c636-c1d6-4bd8-cddb-cc22a7959865"
      },
      "source": [
        "# makes the spectrograms\n",
        "try:\n",
        "  shutil.rmtree('/content/spectrograms')\n",
        "except:\n",
        "  pass\n",
        "os.mkdir('/content/spectrograms')\n",
        "\n",
        "for path in os.listdir('/content/chunks'):\n",
        "    fullPath = os.path.join('/content', 'chunks', path)\n",
        "    audio, sr = librosa.load(fullPath, sr= 44100) \n",
        "    plt.figure(figsize=(8,8.2), dpi= 36)\n",
        "    value = librosa.stft(audio)\n",
        "    data_in_db = librosa.amplitude_to_db(np.abs(value)) #applies Fourier Transform to show power throughout the time\n",
        "    librosa.display.specshow(data_in_db, sr=sr)   #show spectrogram, in terms of log Function because that is how audio is usually represented\n",
        "    \n",
        "    specPath = os.path.join('/content', 'spectrograms', path[:-4] + '.png')\n",
        "    plt.savefig(specPath, format=\"PNG\", bbox_inches= \"tight\", pad_inches= 0, transparent= \"True\", )\n",
        "    plt.clf()\n",
        "    plt.close()\n",
        "    print('Creating spectrogram:', specPath)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating spectrogram: /content/spectrograms/10.png\n",
            "Creating spectrogram: /content/spectrograms/13.png\n",
            "Creating spectrogram: /content/spectrograms/8.png\n",
            "Creating spectrogram: /content/spectrograms/1.png\n",
            "Creating spectrogram: /content/spectrograms/12.png\n",
            "Creating spectrogram: /content/spectrograms/0.png\n",
            "Creating spectrogram: /content/spectrograms/9.png\n",
            "Creating spectrogram: /content/spectrograms/7.png\n",
            "Creating spectrogram: /content/spectrograms/3.png\n",
            "Creating spectrogram: /content/spectrograms/11.png\n",
            "Creating spectrogram: /content/spectrograms/2.png\n",
            "Creating spectrogram: /content/spectrograms/6.png\n",
            "Creating spectrogram: /content/spectrograms/4.png\n",
            "Creating spectrogram: /content/spectrograms/5.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b84zogLq2rwr",
        "outputId": "65c14408-16b1-4c25-bb0f-80bce2551a4e"
      },
      "source": [
        "genres = [0,0,0,0,0]\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "  transforms.Resize([224,224]),\n",
        "  transforms.ToTensor(),\n",
        "  # densenet expects this transformations\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "for spec in os.listdir('/content/spectrograms'):\n",
        "  fullPath = os.path.join('/content', 'spectrograms', spec)\n",
        "  spec = preprocess(Image.open(fullPath).convert('RGB'))\n",
        "  out = model(spec.unsqueeze(0))\n",
        "  pred = out.max(1, keepdim=True)[1][0][0].item()\n",
        "  genres[pred] += 1\n",
        "  \n",
        "print(genres)\n",
        "final_classification = genres.index(max(genres))\n",
        "print(final_classification)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 3, 0, 0, 7]\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}